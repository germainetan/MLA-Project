{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>CNN</h1>\n",
    "\n",
    "References:  \n",
    "https://www.tensorflow.org/tutorials/images/cnn  \n",
    "https://www.datacamp.com/tutorial/cnn-tensorflow-python  \n",
    "\n",
    "Convolutional Layers:  \n",
    "https://www.sciencedirect.com/topics/engineering/convolutional-layer#:~:text=2.3.,-1%20Convolutional%20layer&text=A%20convolutional%20layer%20is%20the,and%20creates%20an%20activation%20map.  \n",
    "https://towardsdatascience.com/convolutional-neural-networks-explained-9cc5188c4939  \n",
    "\n",
    "\n",
    "Model:  \n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/Model  \n",
    "https://keras.io/api/layers/convolution_layers/convolution2d/  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Architecture:\n",
    "\n",
    "Input: 28 x 28 image (Pixel Size)   \n",
    "-> Convolution Layer 1 -> ReLU Activation -> Max Pooling  \n",
    "-> Convolution Layer 2 -> ReLU Activation -> Max Pooling  \n",
    "-> Convolution Layer 3 -> ReLU Activation -> Max Pooling <br/>\n",
    "-> Flatten -> Fully Connected Layer (Dense Layer) -> ReLU Activation  \n",
    "-> Output Layer (17 units)\n",
    "\n",
    "Output Classes:  \n",
    "['(',')','+','-','0','1','2','3','4','5','6','7','8','9','=','div','times'] <br/>\n",
    "Total: 17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Variable (Edit Before you Run on your own)\n",
    "- Important: Change the MODEL_SAVE_NAME before running a new model so that we don't override any saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS_SIZE = 20 #Adjusted for trial first\n",
    "BATCH_SIZE = 128\n",
    "MODEL_SAVE_NAME = \"\" #change this so that u dont overwrite saved model\n",
    "training_class_labels = ['(',')','+','-','0','1','2','3','4','5','6','7','8','9','=','div','times']\n",
    "class_labels_dict = {'(': 0,')': 1,'+': 2,'-': 3,'0': 4,'1': 5,'2': 6,'3': 7,'4': 8,'5': 9,'6': 10,'7': 11,'8': 12,'9': 13,'=': 14,'div': 15,'times': 16}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre requisites that you need to install before use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Just Run Once\n",
    "!pip install tensorflow\n",
    "!pip install scikit-learn\n",
    "!pip install keras\n",
    "!pip install keras-tuner\n",
    "!pip install matplotlib\n",
    "!pip install opencv-python\n",
    "!pip install scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras_tuner as kt\n",
    "import seaborn as sns\n",
    "from keras import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Function to Load The Images from Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing_from_dir(dataset_dir, class_labels_dict, training=False):\n",
    "    # Initialize lists to store images and labels\n",
    "    images = []\n",
    "    labels = []\n",
    "    class_labels = []\n",
    "\n",
    "    # Get a list of all subdirectories (each subdirectory represents a class)\n",
    "    class_directories = os.listdir(dataset_dir)\n",
    "\n",
    "    # Iterate through each subdirectory (class directory)\n",
    "    for class_directory in class_directories:\n",
    "        class_label = class_directory  # Use the directory name as the class label\n",
    "        \n",
    "        class_labels.append(class_label)\n",
    "        class_path = os.path.join(dataset_dir, class_directory)\n",
    "\n",
    "        # Get a list of image files in the class directory\n",
    "        image_files = glob.glob(os.path.join(class_path, \"*.jpg\"))  # You may need to adjust the file extension\n",
    "\n",
    "        # print(image_files)\n",
    "\n",
    "        # Iterate through image files in the class directory\n",
    "        for image_file in image_files:\n",
    "            # Load and preprocess the image\n",
    "            image = cv2.imread(image_file, cv2.IMREAD_GRAYSCALE)\n",
    "            image = cv2.resize(image, (28, 28))\n",
    "            image = image / 255.0  # Normalize pixel values\n",
    "\n",
    "            # plt.imshow(image, cmap=plt.cm.binary)\n",
    "\n",
    "            # Append the preprocessed image and its label to the lists\n",
    "            images.append(image)\n",
    "            labels.append(class_label)\n",
    "\n",
    "    if training:\n",
    "\n",
    "        data = list(zip(images, labels))\n",
    "\n",
    "        # Shuffle the combined data\n",
    "        np.random.shuffle(data)\n",
    "\n",
    "        # shuffle the training images\n",
    "        shuffled_images, shuffled_labels = zip(*data)\n",
    "\n",
    "        images = np.array(shuffled_images)\n",
    "\n",
    "        label_encoder = LabelEncoder()\n",
    "\n",
    "        # Encode class labels using LabelEncoder\n",
    "        labels = label_encoder.fit_transform(shuffled_labels)\n",
    "\n",
    "        for i in range(len(class_labels)):\n",
    "            class_labels_dict[class_labels[i]] = i\n",
    "\n",
    "        labels = np.array(labels, dtype=\"int64\")\n",
    "\n",
    "        # comment the below 2 lines if doing label-encoding\n",
    "        # One-hot encode labels (need to do one code in order to fit into the model)\n",
    "        num_classes = len(class_labels)\n",
    "        labels = to_categorical(labels, num_classes=num_classes)\n",
    "\n",
    "        return images, labels, class_labels, class_labels_dict\n",
    "\n",
    "    else:\n",
    "       \n",
    "        # Convert lists to NumPy arrays\n",
    "        images = np.array(images)\n",
    "\n",
    "        # label-encoding done on test data should correspond to the ones in training data\n",
    "        # this is to account for times when test data is lesser than training data\n",
    "\n",
    "        for i in range(len(labels)):\n",
    "            labels[i] = class_labels_dict[labels[i]]\n",
    "\n",
    "        labels = np.array(labels, dtype=\"int64\")\n",
    "\n",
    "        # comment the below 2 lines if doing label-encoding\n",
    "        # One-hot encode labels (need to do one code in order to fit into the model)\n",
    "        num_classes = len(class_labels_dict)\n",
    "        labels = to_categorical(labels, num_classes=num_classes)\n",
    "\n",
    "        return images, labels, class_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Augmentation Function for Training Data \n",
    "- random rotation\n",
    "- random noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_augmentation(image):\n",
    "\n",
    "    ##############################################################\n",
    "    # Rotating images to mimic slanted handwriting\n",
    "\n",
    "    # Convert the image to a NumPy array (assuming it's in the range [0, 1])\n",
    "    image = (image * 255).astype(np.uint8)\n",
    "\n",
    "    # Calculate the image center\n",
    "    center = tuple(np.array(image.shape[1::-1]) / 2)\n",
    "\n",
    "    rotation_angle = random.uniform(-30, 30)\n",
    "\n",
    "    # Create a rotation matrix and apply the rotation\n",
    "    rotation_matrix = cv2.getRotationMatrix2D(center, rotation_angle, 1.0)\n",
    "    rotated_image = cv2.warpAffine(image, rotation_matrix, image.shape[1::-1], flags=cv2.INTER_LINEAR, borderValue=(255, 255, 255))\n",
    "\n",
    "    # Convert back to the range [0, 1]\n",
    "    rotated_image = rotated_image.astype(np.float32) / 255.0\n",
    "\n",
    "    ##############################################################\n",
    "    # Adding random noise to mimic low quality images\n",
    "\n",
    "    max_noise_level = random.uniform(0, 0.1)\n",
    "    noise = tf.random.normal(shape=tf.shape(rotated_image), stddev=max_noise_level)\n",
    "    \n",
    "    return tf.clip_by_value(rotated_image + noise, 0.0, 1.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Function for Defining The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def math_model(X_train, X_test, y_train, y_test, num_classes, model_name):\n",
    "\n",
    "    # X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "    # perform data augmentation on X_train\n",
    "    X_train = np.array([data_augmentation(image) for image in X_train])\n",
    "\n",
    "    # Define your CNN model for multi-class classification\n",
    "\n",
    "    input_shape = (28,28,1) # decision point: what size are our images fixed at\n",
    "    layer1_size = 32 # number of filters in the convolutional layer\n",
    "    layer2_size = 64\n",
    "    layer3_size = 128\n",
    "    layer_shape = (3,3) # size of the filter\n",
    "\n",
    "    pool_shape = (2,2) # size of the pooling laye\n",
    "    fully_connected_layer_size = 128 # number of neurons in the fully connected layer\n",
    "\n",
    "\n",
    "    model = Sequential([\n",
    "        layers.Conv2D(layer1_size, layer_shape, activation='relu', input_shape=input_shape),\n",
    "        layers.MaxPooling2D(pool_shape),\n",
    "        layers.Conv2D(layer2_size, layer_shape, activation='relu'),\n",
    "        layers.MaxPooling2D(pool_shape),\n",
    "        layers.Conv2D(layer3_size, layer_shape, activation='relu'),\n",
    "        layers.MaxPooling2D(pool_shape),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(fully_connected_layer_size, activation='relu'),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model_metrics = ['accuracy', metrics.Recall(name = \"Recall\"), metrics.Precision(name = \"Precision\")]\n",
    "\n",
    "    # Compile the model\n",
    "    # for one-hot encoding\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=model_metrics)\n",
    "\n",
    "    # uncomment this if using label-encoding, & comment the one above\n",
    "    # model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=model_metrics)\n",
    "\n",
    "    # Fit model. (Batch size either 32, 64, 128. 1000 epochs as we expect training to stop before that.\n",
    "    history = model.fit(X_train, y_train, batch_size=BATCH_SIZE,\n",
    "                    epochs=EPOCHS_SIZE, validation_data=(X_test, y_test))\n",
    "\n",
    "    # Save the trained model for later use\n",
    "    model.save(f\"{model_name}.keras\")\n",
    "\n",
    "    # not a must to return history here but it's to see whether model is overfitting or underfitting after training\n",
    "    # can remove history once we confirmed model is good\n",
    "    return f\"{model_name}.keras\", history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Function for Evaluating Training Model\n",
    "- Confusion Matrix, \n",
    "- Classification Report, \n",
    "- Number of Correct, Wrong and % Match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def math_reports(model, X_test, y_test, test_class_labels, train_class_labels):\n",
    "\n",
    "    # Load the saved model\n",
    "    loaded_model = model + \".keras\"\n",
    "    loaded_model = tf.keras.models.load_model(model)\n",
    "\n",
    "    predicted_y = loaded_model.predict(X_test)\n",
    "\n",
    "    # Convert one-hot encoded labels back to integer labels\n",
    "    y_test_labels = y_test\n",
    "\n",
    "    # comment this if label-encoding was used\n",
    "    y_test_labels = np.argmax(y_test_labels, axis=1)\n",
    "\n",
    "    predicted_labels = np.argmax(predicted_y, axis=1)\n",
    "\n",
    "    confusion = confusion_matrix(y_test_labels, predicted_labels)\n",
    "\n",
    "    cf_report = classification_report(y_test_labels, predicted_labels, labels=np.unique(y_test_labels), target_names=test_class_labels)\n",
    "\n",
    "    # Initialize dictionaries to store correct and total counts for each class\n",
    "    correct_instances_per_class = {}\n",
    "    total_instances_per_class = {}\n",
    "    report = \"\"\n",
    "    predicted_report = \"\"\n",
    "\n",
    "    # Iterate through predictions and true labels to calculate correct and total instances\n",
    "    for i in range(predicted_labels.size):\n",
    "        predicted = train_class_labels[predicted_labels[i]]\n",
    "        test_label = train_class_labels[y_test_labels[i]]\n",
    "\n",
    "        result = \"wrong\"\n",
    "\n",
    "        if (predicted == test_label):\n",
    "            result = \"correct\"\n",
    "\n",
    "        predicted_report += f\"Predicted: {predicted}, Actual: {test_label}, Result: {result}\\n\"\n",
    "\n",
    "        if test_label not in correct_instances_per_class:\n",
    "            correct_instances_per_class[test_label] = 0\n",
    "            total_instances_per_class[test_label] = 0\n",
    "\n",
    "        total_instances_per_class[test_label] += 1\n",
    "\n",
    "        if predicted == test_label:\n",
    "            correct_instances_per_class[test_label] += 1\n",
    "\n",
    "    import operator\n",
    "\n",
    "    sorted_correct = dict(sorted(correct_instances_per_class.items(), key=operator.itemgetter(0)))\n",
    "\n",
    "    # Print the summary of correct/total for each class\n",
    "    for label in sorted_correct:\n",
    "        correct_count = sorted_correct[label]\n",
    "        total_count = total_instances_per_class[label]\n",
    "        report += f\"Class {label}: Correct {correct_count}/{total_count} | Wrong: {total_count - correct_count} | % Match: {round((correct_count/total_count) * 100,2)}\\n\"\n",
    "\n",
    "    # print(report)\n",
    "\n",
    "    return confusion, cf_report, report, predicted_report, predicted_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running model on Training Data (Digits & Symbols Images only (17 classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the directory final_82 is too large, thus it is not stored in the github repo\n",
    "images, labels, training_class_labels, class_labels_dict = pre_processing_from_dir(\"final_82/train_images\", {}, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL_SAVE_NAME is defined in global variable\n",
    "model, history = math_model(X_train, X_test, y_train, y_test, len(training_class_labels), MODEL_SAVE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for under/overfitting & deciding on epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change to the model that you have trained for as the X_test & y_test would be different for each epoch\n",
    "loaded_model = MODEL_SAVE_NAME + \".keras\"\n",
    "loaded_model = tf.keras.models.load_model(loaded_model)\n",
    "\n",
    "loss, accuracy, recall, precision = loaded_model.evaluate(X_test, y_test)\n",
    "print(f\"Loss : {round(loss * 100, 2)}, Accuracy : {round(accuracy * 100, 2)}, Recall : {round(recall * 100, 2)}, Precision : {round(precision * 100, 2)}\")\n",
    "\n",
    "# Plot the training and validation loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plot the training and validation accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reports (Training data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change this variable to get reports on previously trained model\n",
    "loaded_model = MODEL_SAVE_NAME + \".keras\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test class labels & training class labels is the same here\n",
    "confusion, cf_report, report, predicted_report, predicted_labels = math_reports(loaded_model, X_test, y_test, training_class_labels, training_class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Confusion Matrix\")\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification Report\")\n",
    "print(cf_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Confusion Matrix Report\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Predicted vs Actual\")\n",
    "print(predicted_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Model with Unseen Test Data (Digits & Symbols Images only (17 classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the directory final_82 is too large, thus it is not stored in the github repo\n",
    "images_test, labels_test, test_class_labels = pre_processing_from_dir(\"final_82/test_images\", class_labels_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = MODEL_SAVE_NAME + \".keras\"\n",
    "loaded_model = tf.keras.models.load_model(loaded_model)\n",
    "\n",
    "loss, accuracy, recall, precision = loaded_model.evaluate(images_test, labels_test)\n",
    "print(f\"Loss : {round(loss * 100, 2)}, Accuracy : {round(accuracy * 100, 2)}, Recall : {round(recall * 100, 2)}, Precision : {round(precision * 100, 2)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change this variable to get reports on previously trained model\n",
    "loaded_model = MODEL_SAVE_NAME + \".keras\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reports (Unseen Test Data (17 classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict using loaded model\n",
    "confusion_test, cf_report_test, report_test, predicted_report_test, predicted_labels_test = math_reports(loaded_model, images_test, labels_test, test_class_labels, training_class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification Report Test\")\n",
    "print(cf_report_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Confusion Matrix Test\")\n",
    "print(confusion_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to better visualize confusion matrix\n",
    "\n",
    "# Replace this with your class labels\n",
    "class_labels = [key for key, value in class_labels_dict.items() if value in np.unique(predicted_labels_test)]\n",
    "\n",
    "# Create a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.set(font_scale=1.2)\n",
    "sns.heatmap(confusion_test, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)\n",
    "\n",
    "# Set labels and title\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix Heatmap')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Confusion Matrix Report (20 epochs)\")\n",
    "print(report_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Confusion Matrix Report (20t2 epochs)\")\n",
    "print(report_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Confusion Matrix Report (30t2 epochs)\")\n",
    "print(report_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Predicted vs Actual Test\")\n",
    "print(predicted_report_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running model with Unseen Test Data (from Equation Images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing_from_test(dataset_dir, class_label):\n",
    "    # Initialize lists to store images and labels\n",
    "    images = []\n",
    "\n",
    "    # Get a list of all subdirectories (each subdirectory represents a class)\n",
    "    class_directories = os.listdir(dataset_dir)\n",
    "\n",
    "    # look for the directory that matchesthe class_label\n",
    "    for class_directory in class_directories:\n",
    "\n",
    "        if class_label != class_directory:\n",
    "            continue\n",
    "        \n",
    "        class_path = os.path.join(dataset_dir, class_directory)\n",
    "\n",
    "        allowed_extensions = [\"*.jpg\", \"*.jpeg\", \"*.png\"]\n",
    "\n",
    "        image_files = []\n",
    "        for extension in allowed_extensions:\n",
    "            image_files.extend(glob.glob(os.path.join(class_path, extension)))\n",
    "\n",
    "        # Iterate through image files in the class directory\n",
    "        for image_file in image_files:\n",
    "            # Load and preprocess the image\n",
    "            image = cv2.imread(image_file, cv2.IMREAD_GRAYSCALE)\n",
    "            image = cv2.resize(image, (28, 28))\n",
    "            image = image / 255.0  # Normalize pixel values\n",
    "\n",
    "            # plt.imshow(image, cmap=plt.cm.binary)\n",
    "\n",
    "            # Append the preprocessed image and its label to the lists\n",
    "            images.append(image)\n",
    "            # labels.append(class_label)\n",
    "\n",
    "    return np.array(images), class_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve Ground Truth from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file \n",
    "# change to csv that you stored the ground truth\n",
    "filtered_data = pd.read_csv(\"./ground_truth_csv/ten_paths.csv\")\n",
    "\n",
    "filtered_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to dictionary for easy reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to dictionary for easy reference\n",
    "filtered_data_dictionary = filtered_data.set_index('path')['gt'].to_dict()\n",
    "print(filtered_data_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert predicted labels to gt notation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert predicted output to gt notation\n",
    "def combine_all_predict(predicted_labels):\n",
    "    line = \"\"\n",
    "    for i in range(predicted_labels.size):\n",
    "        predicted = training_class_labels[predicted_labels[i]]\n",
    "        line += convert_into_gt(predicted) + \" \"\n",
    "        \n",
    "    return line\n",
    "\n",
    "def convert_into_gt(input_symbol):\n",
    "\n",
    "    # print(input_symbol)\n",
    "    if input_symbol == \"div\":\n",
    "        return \"\\\\div\"\n",
    "    if input_symbol == \"times\":\n",
    "        return \"\\\\times\"\n",
    "    return input_symbol\n",
    "    ## in gt symbals div are \\div and times are \\times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to compare with ground truth and calculate % similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def compare_w_ground(path, combined_predicted, filtered_data_dictionary):\n",
    "    ground_truth = filtered_data_dictionary.get(path)\n",
    "    return string_similarity(combined_predicted, ground_truth)\n",
    "\n",
    "def string_similarity(str1, str2):\n",
    "    # Remove white spaces from both strings\n",
    "    str1 = str1.strip().split(\" \")\n",
    "    str2 = str2.strip()\n",
    "    \n",
    "    pattern = r\"(\\(|\\)|\\\\div|\\\\times|[0-9]|[\\+\\-\\=/]+)\"\n",
    "    # print(re.findall(pattern, str2))\n",
    "    str2 = re.findall(pattern, str2)\n",
    "    \n",
    "    # Calculate the length of the longer string\n",
    "    min_length = min(len(str1), len(str2))\n",
    "    \n",
    "    # Initialize a variable to count the number of matching characters\n",
    "    matching_count = 0\n",
    "\n",
    "    # print(str1)\n",
    "    # print(str2)\n",
    "\n",
    "    for i in range(min_length):\n",
    "        # print(str1[i], str2[i])\n",
    "        if str1[i] == str2[i]:\n",
    "            matching_count += 1\n",
    "    \n",
    "    # Calculate the percentage of similarity\n",
    "    # no. of correct / no. of characters ground truth has\n",
    "    similarity_percentage = (matching_count / len(str2)) * 100\n",
    "    \n",
    "    return round(similarity_percentage, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve predicted ouput & similarity percentage from predicted output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_1 = {}\n",
    "\n",
    "for i in filtered_data_dictionary:\n",
    "    # change folder path here to where the folders of the images are saved in\n",
    "    # e.g., all_equations\n",
    "    ez_test, ez_label = pre_processing_from_test(\"./processed_data/ten_eq\", i)\n",
    "\n",
    "    if ez_test.size != 0:\n",
    "\n",
    "        training_class_labels = ['(',')','+','-','0','1','2','3','4','5','6','7','8','9','=','div','times']\n",
    "\n",
    "        # Load the saved model (change model to load different model)\n",
    "        loaded_model = \"20.keras\"\n",
    "\n",
    "        # loaded_model = tf.keras.saving.load_model(loaded_model)\n",
    "        loaded_model = tf.keras.models.load_model(loaded_model)\n",
    "\n",
    "        predicted_y = loaded_model.predict(ez_test)\n",
    "\n",
    "        predicted_labels = np.argmax(predicted_y, axis=1)\n",
    "\n",
    "        # print(\"predicted_labels: \", predicted_labels)\n",
    "\n",
    "        predicted_report = \"\"\n",
    "\n",
    "        # Iterate through predictions and true labels to calculate correct and total instances\n",
    "        for j in range(predicted_labels.size):\n",
    "            predicted = training_class_labels[predicted_labels[j]]\n",
    "            predicted_report += predicted + \" \"\n",
    "            \n",
    "        # print(\"predicted output: \", predicted_report)\n",
    "        \n",
    "        ## compare with ground truth\n",
    "        combine = combine_all_predict(predicted_labels)\n",
    "\n",
    "        predicted_1[ez_label] = [combine[:-1], compare_w_ground(i, combine, filtered_data_dictionary)]\n",
    "    \n",
    "print(predicted_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store Predicted Output & Percentage Similarity in dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data['predicted output'] = filtered_data['path'].map(lambda x: predicted_1.get(x, [None, None])[0])\n",
    "filtered_data['% similarity'] = filtered_data['path'].map(lambda x: predicted_1.get(x, [None, None])[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate Word Error Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def word_error_rate(reference, hypothesis):\n",
    "    # Tokenize the reference and hypothesis into words\n",
    "    # ground truth\n",
    "    ref_words = reference.strip()\n",
    "    pattern = r\"(\\(|\\)|\\\\div|\\\\times|[0-9]|[\\+\\-\\=/]+)\"\n",
    "    ref_words = re.findall(pattern, ref_words)\n",
    "    \n",
    "    # predicted output\n",
    "    hyp_words = hypothesis.strip().split(\" \")\n",
    "    \n",
    "    # Initialize a 2D matrix to store the edit distances\n",
    "    dp = [[0] * (len(hyp_words) + 1) for _ in range(len(ref_words) + 1)]\n",
    "\n",
    "    # Initialize the first row and first column\n",
    "    for i in range(len(ref_words) + 1):\n",
    "        dp[i][0] = i\n",
    "    for j in range(len(hyp_words) + 1):\n",
    "        dp[0][j] = j\n",
    "\n",
    "    # Fill in the matrix using dynamic programming\n",
    "    for i in range(1, len(ref_words) + 1):\n",
    "        for j in range(1, len(hyp_words) + 1):\n",
    "            cost = 0 if ref_words[i - 1] == hyp_words[j - 1] else 1\n",
    "            dp[i][j] = min(dp[i - 1][j] + 1, dp[i][j - 1] + 1, dp[i - 1][j - 1] + cost)\n",
    "\n",
    "    # The last cell of the matrix contains the minimum edit distance\n",
    "    wer = dp[len(ref_words)][len(hyp_words)]\n",
    "\n",
    "    # Calculate the WER by dividing by the number of words in the reference\n",
    "    wer /= len(ref_words)\n",
    "    #print(len(ref_words))\n",
    "    return wer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code below assumes csv has 2 columns: prediction and groundtruth\n",
    "n = len(filtered_data_dictionary)\n",
    "#Store WER values in a list, then appends it to DataFrame object\n",
    "wer_list = []\n",
    "for i in range(n):\n",
    "    pred = filtered_data.iloc[i][2] #Prediction column\n",
    "    truth = filtered_data.iloc[i][1] #Groundtruth Column\n",
    "    wer = word_error_rate(truth, pred)\n",
    "    if wer == 0:\n",
    "        wer_list.append(0)\n",
    "    else:\n",
    "        wer_list.append(round(wer,2))\n",
    "        \n",
    "filtered_data['WER'] = wer_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save results to evaluation csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change accordingly to how you would like your file name to be named\n",
    "filtered_data.to_csv(\"./evaluations_csv/fair_ten.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
